Let $X$ and $Y$ be random variables. The mutual information is 
$$
I(X;Y)=H(X)-H(X|Y)
$$
i.e. the amount of information about $X$ conveyed by $Y$
(where $H$ is the [[Mathematical Entropy]])
Note $I(X;Y)\geq 0$ with equality iff $X$ and $Y$ are independent.
Also $I(X;Y)=I(Y;X)$

