Entropy is a measure of 'randomness' or 'uncertainty'

Consider a random variable $X$ taking values $x_{1}\dots x_{n}$ with probabilities $p_{1},\dots,p_{n}$. The entropy $H(X)$ is roughly speaking the expected number of tosses of a fair coin 